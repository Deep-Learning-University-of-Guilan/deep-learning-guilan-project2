{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A2x-eRJc6VTZ"
      },
      "source": [
        "# Importing Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8wXLgz_EYiFr"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split, KFold\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_recall_fscore_support\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "\n",
        "!gdown 1-Zyp-JP3f9QhPKaErBkPPFNKaPS1v74u"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zGOep0Rm6bMy"
      },
      "source": [
        "# Data Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TDpnh7Jl6jHZ"
      },
      "source": [
        "## Importing data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 592
        },
        "id": "lirSlJSTYqd6",
        "outputId": "68f04b69-f090-41d9-cf4e-c036fb3152a7"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('/content/Dataset.csv')\n",
        "df.loc[df[\"Class\"] == \"L\", \"Class\"] = 0\n",
        "df.loc[df[\"Class\"] == \"M\", \"Class\"] = 1\n",
        "df.loc[df[\"Class\"] == \"H\", \"Class\"] = 2\n",
        "display(df)\n",
        "print(df.columns)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F5F3eEwA6tWh"
      },
      "source": [
        "## Columns' analysis\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m0PYCaE3DMmZ"
      },
      "source": [
        "### Gender\n",
        "Here we can see the distribution of genders in dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "Yh7dujHU5kxk",
        "outputId": "104d57e6-3cf8-429e-8a9c-ff8d2d1953cb"
      },
      "outputs": [],
      "source": [
        "ax = sns.countplot(data=df, x=\"gender\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UI9nrUq97TuX"
      },
      "source": [
        "we can see that there are more male record than female \\\n",
        "now we check how the gender affects the performance of students"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "uBN9XHcr7yMN",
        "outputId": "f0932ec3-590c-4058-a90e-aa53a5a1aa43"
      },
      "outputs": [],
      "source": [
        "ax = sns.countplot(data=df, x=\"gender\")\n",
        "gender = df['gender'].unique()\n",
        "gender_avg = [sum(df[df['gender'] == i].Class)/float(len(df[df['gender'] == i])) for i in gender]\n",
        "ax = sns.barplot(x=gender, y=gender_avg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LjSNycbPC_ma"
      },
      "source": [
        "so maybe female students are more intelligent!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jQQsavGUDCTD"
      },
      "source": [
        "### NationalITy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 365
        },
        "id": "UxYd0Dmv_TnO",
        "outputId": "4675a1fd-96f4-41cc-bdfe-dc6eee5ddea7"
      },
      "outputs": [],
      "source": [
        "ax = sns.countplot(data=df, x=\"NationalITy\")\n",
        "plt.xticks(rotation=90)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IlbWl8xQDhZA"
      },
      "source": [
        "the major number of students are from Jordan and KW \\\n",
        "Do the nationality affect the performance?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 351
        },
        "id": "EdLTCbdEDxuD",
        "outputId": "87304d4a-9018-4f34-d1e7-3745f75e09da"
      },
      "outputs": [],
      "source": [
        "nationality = df['NationalITy'].unique()\n",
        "nationality_avg = [sum(df[df['NationalITy'] == i].Class)/float(len(df[df['NationalITy'] == i])) for i in nationality]\n",
        "ax = sns.barplot(x=nationality, y=nationality_avg)\n",
        "plt.xticks(rotation=90)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VLaGY8wKEeKH"
      },
      "source": [
        "### PlaceofBirth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 365
        },
        "id": "wUNhhZPHEeKH",
        "outputId": "676a1f3b-a1d5-413e-c827-680dc7314e33"
      },
      "outputs": [],
      "source": [
        "ax = sns.countplot(data=df, x=\"PlaceofBirth\")\n",
        "plt.xticks(rotation=90)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HvpW-O1UEeKI"
      },
      "source": [
        "Same as nationality \\"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TYBiVdaSKPW4"
      },
      "source": [
        "### StageID"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "l7GubIQuKWtg",
        "outputId": "b1a768b8-424e-40e3-ea25-c214567a7c28"
      },
      "outputs": [],
      "source": [
        "ax = sns.countplot(data=df, x=\"StageID\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bz7lRV0JTIGZ"
      },
      "source": [
        "### Topic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 386
        },
        "id": "uEKSVZFGoELh",
        "outputId": "8bfd9182-0d06-4b3c-b337-372ee0f393a1"
      },
      "outputs": [],
      "source": [
        "sns.displot(df, x=\"Topic\",height=5, aspect=2.5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bQBxPqinPuSR"
      },
      "source": [
        "### Relation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "UsL3ci5aPpRD",
        "outputId": "ba200d99-e926-4fe3-d506-7d77ef9e917c"
      },
      "outputs": [],
      "source": [
        "ax = sns.countplot(data=df, x=\"Relation\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "b7lD5ccPP6kx",
        "outputId": "a61318b9-8fe8-4dfd-c040-5beda025a4b9"
      },
      "outputs": [],
      "source": [
        "relation = df['Relation'].unique()\n",
        "relation_avg = [sum(df[df['Relation'] == i].Class)/float(len(df[df['Relation'] == i])) for i in relation]\n",
        "ax = sns.barplot(x=relation, y=relation_avg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nIV9f0K1QLPE"
      },
      "source": [
        "so students who leave with their Mum have more performance!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n50_HfA0Qfuk"
      },
      "source": [
        "### raisedhands"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 369
        },
        "id": "WqYkqpMoQfY3",
        "outputId": "1517d689-9219-4fe4-e4ce-53c20afb8782"
      },
      "outputs": [],
      "source": [
        "ax = sns.displot(data=df, x=\"raisedhands\", hue=\"Class\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 743
        },
        "id": "6XlrUMKRQJUG",
        "outputId": "908b5036-dfe6-4c15-dbbf-c8601ff48b99"
      },
      "outputs": [],
      "source": [
        "sns.pairplot(df,hue='Class')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "187QHWrVTllE"
      },
      "source": [
        "# Deep Learning Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bZ4XzO3HTsBm"
      },
      "source": [
        "## Clean up the data for modeling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f0aAd3yb5fpy",
        "outputId": "cd6063aa-c063-4012-cdda-2ffaf4c762bd"
      },
      "outputs": [],
      "source": [
        "y = np.array(df['Class'])\n",
        "X = np.array(pd.get_dummies(df.iloc[:, :-1])).astype(\"float\")\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "\n",
        "normalizer = StandardScaler()\n",
        "X_train = normalizer.fit_transform(X_train)\n",
        "X_test = normalizer.transform(X_test)\n",
        "\n",
        "print(\"Len Train Data:\", len(X_train))\n",
        "print(\"Len Test Data:\", len(X_test))\n",
        "\n",
        "input_features = X_train.shape[1]\n",
        "print(\"number of features:\", input_features)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8U-LcY9SVvgH"
      },
      "source": [
        "## Create Dataset and Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WrvvhoNxYtkt"
      },
      "outputs": [],
      "source": [
        "class Dataset(torch.utils.data.Dataset):\n",
        "  def __init__(self, X, y):\n",
        "    self.X = X.astype(np.float32)\n",
        "    self.y = y\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.X)\n",
        "\n",
        "  def __getitem__(self, item):\n",
        "    return self.X[item], self.y[item]\n",
        "\n",
        "BATCH_SIZE = 16\n",
        "\n",
        "trainset = Dataset(X_train, y_train)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=BATCH_SIZE)\n",
        "\n",
        "testset = Dataset(X_test, y_test)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=BATCH_SIZE)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ErP5CAWEVz9L"
      },
      "source": [
        "## Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iRrpxaGHV4MD"
      },
      "source": [
        "### Needed functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F9h78iIFeYhb"
      },
      "outputs": [],
      "source": [
        "def train_ep(dataloader, model, loss_fn, optimizer, device, n_all):\n",
        "  all = len(dataloader)\n",
        "  losses = []\n",
        "  corrects = 0\n",
        "  model.train()\n",
        "  n = 0\n",
        "  for X, y in dataloader:\n",
        "\n",
        "    X = X.to(device)\n",
        "    y = y.to(device)\n",
        "\n",
        "    outputs = model(X)\n",
        "\n",
        "    _, pred = torch.max(outputs, dim=1)\n",
        "    corrects += torch.sum(pred == y)\n",
        "\n",
        "    loss = loss_fn(outputs, y)\n",
        "    losses.append(loss.item())\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "    \n",
        "    n += 1\n",
        "    # print(f\"batch {n} / {all} -- loss: {loss.item():.5f}\")\n",
        "  return corrects.item() / n_all  * 100, np.mean(losses)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "etVhiJxi_MBH"
      },
      "outputs": [],
      "source": [
        "def validation_ep(dataloader, model, loss_fn, device, n_all):\n",
        "  all = len(dataloader)\n",
        "  losses = []\n",
        "  loss = 0\n",
        "  corrects = 0\n",
        "  n = 0\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    for X, y in dataloader:\n",
        "      X = X.to(device)\n",
        "      y = y.to(device)\n",
        "\n",
        "      outputs = model(X)\n",
        "      _, pred = torch.max(outputs, dim=1)\n",
        "      corrects += torch.sum(pred == y)\n",
        "\n",
        "      loss = loss_fn(outputs, y)\n",
        "      losses.append(loss.item())\n",
        "      \n",
        "      n += 1\n",
        "      # print(f\"batch {n} / {all} -- loss: {loss.item():.5f}\")\n",
        "  return (corrects.item() / n_all) * 100, np.mean(losses)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nOYNYfMEe-Rj"
      },
      "outputs": [],
      "source": [
        "def fit(model, loss_fn, optimizer, EPOCHS):\n",
        "  losses_valid = []\n",
        "  losses_train = []\n",
        "  acc_valid = []\n",
        "  acc_train = []\n",
        "  best_acc = 0\n",
        "\n",
        "  for i in range(EPOCHS):\n",
        "      train_c, train_l = train_ep(trainloader, model, loss_fn, optimizer, device, len(trainset))\n",
        "      print(f\"Epoch {i} ------ train accuracy {train_c:.3f}    train losses {train_l:.3f}\", end='')\n",
        "      losses_train.append(train_l)  \n",
        "      acc_train.append(train_c)  \n",
        "\n",
        "      val_c, val_l = validation_ep(testloader, model, loss_fn, device, len(testset))\n",
        "      print(f\"    valid accuracy {val_c:.3f}    valid losses {val_l:.3f}\")\n",
        "      losses_valid.append(val_l)\n",
        "      acc_valid.append(val_c)\n",
        "  return losses_valid, losses_train, acc_valid, acc_train  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "za-6a0MgOrlv"
      },
      "outputs": [],
      "source": [
        "def plotplz(filename=None):\n",
        "  fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20,10))\n",
        "  # ax1.title(\"Training and Validation Accuracy\")\n",
        "  ax1.plot(acc_valid,label=\"val\")\n",
        "  ax1.plot(acc_train,label=\"train\")\n",
        "  ax1.set_xlabel(\"iterations\")\n",
        "  ax1.set_ylabel(\"accuracy\")\n",
        "  ax1.legend()\n",
        "\n",
        "  ax2.plot(losses_valid,label=\"val\")\n",
        "  ax2.plot(losses_train,label=\"train\")\n",
        "  ax2.set_xlabel(\"iterations\")\n",
        "  ax2.set_ylabel(\"loss\")\n",
        "  ax2.legend()\n",
        "  if filename:\n",
        "    fig.savefig(filename)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RalyAeQ1UKB3"
      },
      "outputs": [],
      "source": [
        "def show_metrics(model):\n",
        "  y_pred = []\n",
        "  y_true = []\n",
        "  with torch.no_grad():\n",
        "    for X, y in testloader:\n",
        "      outputs = model(X.to(device))\n",
        "      _, pred = torch.max(outputs, dim=1)\n",
        "      y_pred.append(pred)\n",
        "      y_true.append(y)\n",
        "  y_pred = torch.cat(y_pred).cpu()\n",
        "  y_true = torch.cat(y_true)\n",
        "  precision, recall, f1score, _ = precision_recall_fscore_support(y_true, y_pred, average='macro')\n",
        "  print(f\"precision: {precision}\\nrecall: {recall}\\nf1 score: {f1score}\")\n",
        "  disp = ConfusionMatrixDisplay(confusion_matrix(y_true, y_pred))\n",
        "  disp.plot()\n",
        "  plt.savefig('conf.png')\n",
        "  return precision, recall, f1score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w1jAtzQHPEzv"
      },
      "source": [
        "### 3 Hidden Layer - ReLU - Adam"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SlZhcUuvLyyk",
        "outputId": "121fa586-b94c-4ee8-8c1a-2fd47587480c"
      },
      "outputs": [],
      "source": [
        "class NN3layer(nn.Module):\n",
        "  def __init__(self, input_features, neurons):\n",
        "    super().__init__()\n",
        "    self.neurons = neurons\n",
        "    self.layers = nn.Sequential(\n",
        "        nn.Linear(input_features, self.neurons),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(self.neurons, self.neurons),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(self.neurons, self.neurons),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(self.neurons, 3),\n",
        "    )\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.layers(x)\n",
        "\n",
        "model = NN3layer(input_features, 128).to(device)\n",
        "criterion = nn.CrossEntropyLoss().to(device)\n",
        "\n",
        "LR, WEIGHT_DECAY = 5e-5, 0\n",
        "optimizer = torch.optim.Adam(\n",
        "    model.parameters(),\n",
        "    lr=LR,\n",
        "    weight_decay=WEIGHT_DECAY\n",
        ")\n",
        "losses_valid, losses_train, acc_valid, acc_train = fit(model, criterion, optimizer, 300)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 877
        },
        "id": "pJIPpwMEfeW0",
        "outputId": "3430fad9-b2db-4197-a57a-8a9db40d953e"
      },
      "outputs": [],
      "source": [
        "plotplz(\"3layer.png\")\n",
        "show_metrics(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6YtZCrlt5v6l"
      },
      "source": [
        "### 4 Hidden layer - ReLU - Adam"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g8M7J2dn57Af",
        "outputId": "5660128e-3c83-4c0f-87fe-6d3acb279587"
      },
      "outputs": [],
      "source": [
        "class NN4layer(nn.Module):\n",
        "  def __init__(self, input_features, neurons):\n",
        "    super().__init__()\n",
        "    self.neurons = neurons\n",
        "    self.layers = nn.Sequential(\n",
        "        nn.Linear(input_features, self.neurons),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(self.neurons, self.neurons),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(self.neurons, self.neurons),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(self.neurons, self.neurons),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(self.neurons, 3),\n",
        "    )\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.layers(x)\n",
        "\n",
        "model = NN4layer(input_features, 128).to(device)\n",
        "criterion = nn.CrossEntropyLoss().to(device)\n",
        "\n",
        "LR, WEIGHT_DECAY = 5e-5, 0\n",
        "optimizer = torch.optim.Adam(\n",
        "    model.parameters(),\n",
        "    lr=LR,\n",
        "    weight_decay=WEIGHT_DECAY\n",
        ")\n",
        "losses_valid, losses_train, acc_valid, acc_train = fit(model, criterion, optimizer, 300)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 881
        },
        "id": "4ruG7Iv357Af",
        "outputId": "77a19605-cb08-4d6f-b0f2-0bac9b9a43f8"
      },
      "outputs": [],
      "source": [
        "plotplz(\"test.png\")\n",
        "show_metrics(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IHm7CwPr-PIr"
      },
      "source": [
        "### 5 Hidden Layer - ReLU - Adam"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "02HdqAl6-U-p",
        "outputId": "50d63c86-9f60-47d9-d1ae-2055fa37b65f"
      },
      "outputs": [],
      "source": [
        "class NN5layer(nn.Module):\n",
        "  def __init__(self, input_features, neurons):\n",
        "    super().__init__()\n",
        "    self.neurons = neurons\n",
        "    self.layers = nn.Sequential(\n",
        "        nn.Linear(input_features, self.neurons),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(self.neurons, self.neurons),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(self.neurons, self.neurons),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(self.neurons, self.neurons),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(self.neurons, self.neurons),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(self.neurons, 3),\n",
        "    )\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.layers(x)\n",
        "\n",
        "model = NN5layer(input_features, 128).to(device)\n",
        "criterion = nn.CrossEntropyLoss().to(device)\n",
        "\n",
        "LR, WEIGHT_DECAY = 5e-5, 0\n",
        "optimizer = torch.optim.Adam(\n",
        "    model.parameters(),\n",
        "    lr=LR,\n",
        "    weight_decay=WEIGHT_DECAY\n",
        ")\n",
        "losses_valid, losses_train, acc_valid, acc_train = fit(model, criterion, optimizer, 300)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 877
        },
        "id": "OjwAbHh4-U-p",
        "outputId": "d9ffd805-dac3-4d92-ad6b-706307897fef"
      },
      "outputs": [],
      "source": [
        "plotplz(\"test.png\")\n",
        "show_metrics(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M4IolXTcCZ42"
      },
      "source": [
        "### 4 Hidden Layer - ReLU - Adam - Dropout"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d8a7OPz7DLcd",
        "outputId": "203688e4-9ce3-47a5-e955-27a807314f89"
      },
      "outputs": [],
      "source": [
        "class NN4layerDrop(nn.Module):\n",
        "  def __init__(self, input_features, neurons):\n",
        "    super().__init__()\n",
        "    self.neurons = neurons\n",
        "    self.layers = nn.Sequential(\n",
        "        nn.Linear(input_features, self.neurons),\n",
        "        nn.ReLU(),\n",
        "        nn.Dropout(0.2),\n",
        "        nn.Linear(self.neurons, self.neurons),\n",
        "        nn.ReLU(),\n",
        "        nn.Dropout(0.2),\n",
        "        nn.Linear(self.neurons, self.neurons),\n",
        "        nn.ReLU(),\n",
        "        nn.Dropout(0.2),\n",
        "        nn.Linear(self.neurons, self.neurons),\n",
        "        nn.ReLU(),\n",
        "        nn.Dropout(0.2),\n",
        "        nn.Linear(self.neurons, 3),\n",
        "    )\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.layers(x)\n",
        "\n",
        "model = NN4layerDrop(input_features, 128).to(device)\n",
        "criterion = nn.CrossEntropyLoss().to(device)\n",
        "\n",
        "LR, WEIGHT_DECAY = 5e-5, 0\n",
        "optimizer = torch.optim.Adam(\n",
        "    model.parameters(),\n",
        "    lr=LR,\n",
        "    weight_decay=WEIGHT_DECAY\n",
        ")\n",
        "losses_valid, losses_train, acc_valid, acc_train = fit(model, criterion, optimizer, 600)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 912
        },
        "id": "xbpXEGtHDLce",
        "outputId": "d9ffd805-dac3-4d92-ad6b-706307897fef"
      },
      "outputs": [],
      "source": [
        "plotplz(\"test.png\")\n",
        "show_metrics(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X70uibcJFadc"
      },
      "source": [
        "### 4 Hidden Layer - ReLU - SGD - Dropout"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T9k9d9PpFfMl",
        "outputId": "41ae9d97-15a8-4a67-a90d-8a79db2f7bbc"
      },
      "outputs": [],
      "source": [
        "class NN4layerDrop(nn.Module):\n",
        "  def __init__(self, input_features, neurons):\n",
        "    super().__init__()\n",
        "    self.neurons = neurons\n",
        "    self.layers = nn.Sequential(\n",
        "        nn.Linear(input_features, self.neurons),\n",
        "        nn.ReLU(),\n",
        "        nn.Dropout(0.2),\n",
        "        nn.Linear(self.neurons, self.neurons),\n",
        "        nn.ReLU(),\n",
        "        nn.Dropout(0.2),\n",
        "        nn.Linear(self.neurons, self.neurons),\n",
        "        nn.ReLU(),\n",
        "        nn.Dropout(0.2),\n",
        "        nn.Linear(self.neurons, self.neurons),\n",
        "        nn.ReLU(),\n",
        "        nn.Dropout(0.2),\n",
        "        nn.Linear(self.neurons, 3),\n",
        "    )\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.layers(x)\n",
        "\n",
        "model = NN4layerDrop(input_features, 128).to(device)\n",
        "criterion = nn.CrossEntropyLoss().to(device)\n",
        "\n",
        "LR, WEIGHT_DECAY = 1e-2, 0\n",
        "optimizer = torch.optim.SGD(\n",
        "    model.parameters(),\n",
        "    lr=LR,\n",
        "    weight_decay=WEIGHT_DECAY\n",
        ")\n",
        "losses_valid, losses_train, acc_valid, acc_train = fit(model, criterion, optimizer, 600)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 877
        },
        "id": "0mDi0yG0FfMl",
        "outputId": "2eb1ce94-5ac5-4b4b-ce65-d343b9f2cb0e"
      },
      "outputs": [],
      "source": [
        "plotplz(\"test.png\")\n",
        "show_metrics(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mlSDqM6mGYdM"
      },
      "source": [
        "### 4 Hidden Layer - Sigmooid - Adam - Dropout"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VbjXKDYNHCdc",
        "outputId": "2e452815-7605-4c5d-aa83-2af4085b3205"
      },
      "outputs": [],
      "source": [
        "class NN4layerDrop(nn.Module):\n",
        "  def __init__(self, input_features, neurons):\n",
        "    super().__init__()\n",
        "    self.neurons = neurons\n",
        "    self.layers = nn.Sequential(\n",
        "        nn.Linear(input_features, self.neurons),\n",
        "        nn.Dropout(0.3),\n",
        "        nn.Sigmoid(),\n",
        "        nn.Linear(self.neurons, self.neurons),\n",
        "        nn.Dropout(0.3),\n",
        "        nn.Sigmoid(),\n",
        "        nn.Linear(self.neurons, self.neurons),\n",
        "        nn.Dropout(0.3),\n",
        "        nn.Sigmoid(),\n",
        "        nn.Linear(self.neurons, self.neurons),\n",
        "        nn.Dropout(0.3),\n",
        "        nn.Sigmoid(),\n",
        "        nn.Linear(self.neurons, 3),\n",
        "    )\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.layers(x)\n",
        "\n",
        "model = NN4layerDrop(input_features, 128).to(device)\n",
        "criterion = nn.CrossEntropyLoss().to(device)\n",
        "\n",
        "LR, WEIGHT_DECAY = 1e-2, 0\n",
        "optimizer = torch.optim.Adam(\n",
        "    model.parameters(),\n",
        "    lr=LR,\n",
        "    weight_decay=WEIGHT_DECAY\n",
        ")\n",
        "losses_valid, losses_train, acc_valid, acc_train = fit(model, criterion, optimizer, 600)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 936
        },
        "id": "EGgWoKbVHCdd",
        "outputId": "5c4694f3-b735-4729-f515-60d0f8224234"
      },
      "outputs": [],
      "source": [
        "plotplz(\"test.png\")\n",
        "show_metrics(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gz3X7clPI-Ro"
      },
      "source": [
        "### 4 Hidden Layer - Tanh - Adam - Dropout"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dTpzVf56KVLs",
        "outputId": "dce5ad82-8c53-4350-f387-90acb8648d0b"
      },
      "outputs": [],
      "source": [
        "class NN4layerDrop(nn.Module):\n",
        "  def __init__(self, input_features, neurons):\n",
        "    super().__init__()\n",
        "    self.neurons = neurons\n",
        "    self.layers = nn.Sequential(\n",
        "        nn.Linear(input_features, self.neurons),\n",
        "        nn.Dropout(0.3),\n",
        "        nn.Tanh(),\n",
        "        nn.Linear(self.neurons, self.neurons),\n",
        "        nn.Dropout(0.3),\n",
        "        nn.Tanh(),\n",
        "        nn.Linear(self.neurons, self.neurons),\n",
        "        nn.Dropout(0.3),\n",
        "        nn.Tanh(),\n",
        "        nn.Linear(self.neurons, self.neurons),\n",
        "        nn.Dropout(0.3),\n",
        "        nn.Tanh(),\n",
        "        nn.Linear(self.neurons, 3),\n",
        "    )\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.layers(x)\n",
        "\n",
        "model = NN4layerDrop(input_features, 128).to(device)\n",
        "criterion = nn.CrossEntropyLoss().to(device)\n",
        "\n",
        "LR, WEIGHT_DECAY = 1e-4, 0\n",
        "optimizer = torch.optim.Adam(\n",
        "    model.parameters(),\n",
        "    lr=LR,\n",
        "    weight_decay=WEIGHT_DECAY\n",
        ")\n",
        "losses_valid, losses_train, acc_valid, acc_train = fit(model, criterion, optimizer, 600)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 877
        },
        "id": "Mh1daewBKVLs",
        "outputId": "84ee5fe8-de9c-4743-baf4-0f44b7185800"
      },
      "outputs": [],
      "source": [
        "plotplz(\"4drop.png\")\n",
        "show_metrics(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tmIGhiZ1Q8MU"
      },
      "source": [
        "### 4 Hidden Layer - ReLU - Adam - BatchNorm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "joaQoUZyRFbT",
        "outputId": "187010ee-25f1-4b05-8735-19a745ae2cf1"
      },
      "outputs": [],
      "source": [
        "class NN4layerNorm(nn.Module):\n",
        "  def __init__(self, input_features, neurons):\n",
        "    super().__init__()\n",
        "    self.neurons = neurons\n",
        "    self.layers = nn.Sequential(\n",
        "        nn.Linear(input_features, self.neurons),\n",
        "        nn.BatchNorm1d(self.neurons),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(self.neurons, self.neurons),\n",
        "        nn.BatchNorm1d(self.neurons),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(self.neurons, self.neurons),\n",
        "        nn.BatchNorm1d(self.neurons),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(self.neurons, self.neurons),\n",
        "        nn.BatchNorm1d(self.neurons),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(self.neurons, 3),\n",
        "    )\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.layers(x)\n",
        "\n",
        "model = NN4layerNorm(input_features, 128).to(device)\n",
        "criterion = nn.CrossEntropyLoss().to(device)\n",
        "\n",
        "LR, WEIGHT_DECAY = 1e-4, 0\n",
        "optimizer = torch.optim.Adam(\n",
        "    model.parameters(),\n",
        "    lr=LR,\n",
        "    weight_decay=WEIGHT_DECAY\n",
        ")\n",
        "losses_valid, losses_train, acc_valid, acc_train = fit(model, criterion, optimizer, 600)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 878
        },
        "id": "lCUZKVy7RFbT",
        "outputId": "66ee3b51-2ff7-44b6-9056-dcc896844c54"
      },
      "outputs": [],
      "source": [
        "plotplz(\"4norm.png\")\n",
        "show_metrics(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i_7sz_wDXtcl"
      },
      "source": [
        "### 3 Hidden Layer - ReLU - Adam - BatchNorm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e5duYGRwZDA0",
        "outputId": "187010ee-25f1-4b05-8735-19a745ae2cf1"
      },
      "outputs": [],
      "source": [
        "class NN4layerNorm(nn.Module):\n",
        "  def __init__(self, input_features, neurons):\n",
        "    super().__init__()\n",
        "    self.neurons = neurons\n",
        "    self.layers = nn.Sequential(\n",
        "        nn.Linear(input_features, self.neurons),\n",
        "        nn.BatchNorm1d(self.neurons),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(self.neurons, self.neurons),\n",
        "        nn.BatchNorm1d(self.neurons),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(self.neurons, self.neurons),\n",
        "        nn.BatchNorm1d(self.neurons),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(self.neurons, 3),\n",
        "    )\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.layers(x)\n",
        "\n",
        "model = NN4layerNorm(input_features, 128).to(device)\n",
        "criterion = nn.CrossEntropyLoss().to(device)\n",
        "\n",
        "LR, WEIGHT_DECAY = 1e-4, 0\n",
        "optimizer = torch.optim.Adam(\n",
        "    model.parameters(),\n",
        "    lr=LR,\n",
        "    weight_decay=WEIGHT_DECAY\n",
        ")\n",
        "losses_valid, losses_train, acc_valid, acc_train = fit(model, criterion, optimizer, 600)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 878
        },
        "id": "GAISjLmyZDA1",
        "outputId": "66ee3b51-2ff7-44b6-9056-dcc896844c54"
      },
      "outputs": [],
      "source": [
        "plotplz(\"4norm.png\")\n",
        "show_metrics(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gNafR4sVZD6U"
      },
      "source": [
        "### 4 Hidden Layer - ReLU - Adam WD - BatchNorm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H48oPvVEZPGQ",
        "outputId": "f59bfad7-1c9c-44fe-ac3c-7b555813281c"
      },
      "outputs": [],
      "source": [
        "class NN4layerNorm(nn.Module):\n",
        "  def __init__(self, input_features, neurons):\n",
        "    super().__init__()\n",
        "    self.neurons = neurons\n",
        "    self.layers = nn.Sequential(\n",
        "        nn.Linear(input_features, self.neurons),\n",
        "        nn.BatchNorm1d(self.neurons),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(self.neurons, self.neurons),\n",
        "        nn.BatchNorm1d(self.neurons),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(self.neurons, self.neurons),\n",
        "        nn.BatchNorm1d(self.neurons),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(self.neurons, self.neurons),\n",
        "        nn.BatchNorm1d(self.neurons),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(self.neurons, 3),\n",
        "    )\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.layers(x)\n",
        "\n",
        "model = NN4layerNorm(input_features, 128).to(device)\n",
        "criterion = nn.CrossEntropyLoss().to(device)\n",
        "\n",
        "LR, WEIGHT_DECAY = 1e-5, 1e-3\n",
        "optimizer = torch.optim.Adam(\n",
        "    model.parameters(),\n",
        "    lr=LR,\n",
        "    weight_decay=WEIGHT_DECAY\n",
        ")\n",
        "losses_valid, losses_train, acc_valid, acc_train = fit(model, criterion, optimizer, 600)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 878
        },
        "id": "Dx0S-DbcZPGS",
        "outputId": "66ee3b51-2ff7-44b6-9056-dcc896844c54"
      },
      "outputs": [],
      "source": [
        "plotplz(\"4norm.png\")\n",
        "show_metrics(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Odei6xMZrhll"
      },
      "source": [
        "### 4 Hidden layer - Tanh - Relu - Adam - BatchNorm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IQrm8PRgro2C",
        "outputId": "393d0867-84b5-43c0-a04b-a07b22c0f24d"
      },
      "outputs": [],
      "source": [
        "class NN4layerNorm(nn.Module):\n",
        "  def __init__(self, input_features, neurons):\n",
        "    super().__init__()\n",
        "    self.neurons = neurons\n",
        "    self.layers = nn.Sequential(\n",
        "        nn.Linear(input_features, self.neurons),\n",
        "        nn.BatchNorm1d(self.neurons),\n",
        "        nn.Tanh(),\n",
        "        nn.Linear(self.neurons, self.neurons),\n",
        "        nn.BatchNorm1d(self.neurons),\n",
        "        nn.Tanh(),\n",
        "        nn.Linear(self.neurons, self.neurons),\n",
        "        nn.BatchNorm1d(self.neurons),\n",
        "        nn.Tanh(),\n",
        "        nn.Linear(self.neurons, self.neurons),\n",
        "        nn.BatchNorm1d(self.neurons),\n",
        "        nn.Tanh(),\n",
        "        nn.Linear(self.neurons, self.neurons),\n",
        "        nn.BatchNorm1d(self.neurons),\n",
        "        nn.Tanh(),\n",
        "        nn.Linear(self.neurons, 3),\n",
        "    )\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.layers(x)\n",
        "\n",
        "model = NN4layerNorm(input_features, 128).to(device)\n",
        "criterion = nn.CrossEntropyLoss().to(device)\n",
        "\n",
        "LR, WEIGHT_DECAY = 1e-5, 1e-8\n",
        "optimizer = torch.optim.Adam(\n",
        "    model.parameters(),\n",
        "    lr=LR,\n",
        "    weight_decay=WEIGHT_DECAY\n",
        ")\n",
        "losses_valid, losses_train, acc_valid, acc_train = fit(model, criterion, optimizer, 600)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 909
        },
        "id": "sltOirclro2C",
        "outputId": "75dd2832-f923-4122-d654-50168edd5f77"
      },
      "outputs": [],
      "source": [
        "plotplz(\"4normtanh.png\")\n",
        "show_metrics(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Y4_olqEcr2d"
      },
      "source": [
        "# Cross Validation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xfHUWRZd2px_"
      },
      "source": [
        "## Model 1:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "g0C-1ZlXDntn",
        "outputId": "aa16ee86-89a4-4cbc-aef6-b3ad0a33e6fb"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 16\n",
        "\n",
        "dataset = Dataset(X, y)\n",
        "criterion = nn.CrossEntropyLoss().to(device)\n",
        "\n",
        "class NN4layerDrop(nn.Module):\n",
        "  def __init__(self, input_features, neurons):\n",
        "    super().__init__()\n",
        "    self.neurons = neurons\n",
        "    self.layers = nn.Sequential(\n",
        "        nn.Linear(input_features, self.neurons),\n",
        "        nn.Dropout(0.3),\n",
        "        nn.Tanh(),\n",
        "        nn.Linear(self.neurons, self.neurons),\n",
        "        nn.Dropout(0.3),\n",
        "        nn.Tanh(),\n",
        "        nn.Linear(self.neurons, self.neurons),\n",
        "        nn.Dropout(0.3),\n",
        "        nn.Tanh(),\n",
        "        nn.Linear(self.neurons, self.neurons),\n",
        "        nn.Dropout(0.3),\n",
        "        nn.Tanh(),\n",
        "        nn.Linear(self.neurons, 3),\n",
        "    )\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.layers(x)\n",
        "\n",
        "foldperf={}\n",
        "\n",
        "kfold = KFold(n_splits=5, shuffle=True)\n",
        "for fold, (train_ids, test_ids) in enumerate(kfold.split(dataset)):\n",
        "  print(f\"----------------- Fold {fold+1} --------------------\")\n",
        "  train_subsampler = torch.utils.data.SubsetRandomSampler(train_ids)\n",
        "  test_subsampler = torch.utils.data.SubsetRandomSampler(test_ids)\n",
        "  \n",
        "  trainloader = torch.utils.data.DataLoader(\n",
        "                    dataset, \n",
        "                    batch_size=BATCH_SIZE, sampler=train_subsampler)\n",
        "  testloader = torch.utils.data.DataLoader(\n",
        "                    dataset,\n",
        "                    batch_size=BATCH_SIZE, sampler=test_subsampler)\n",
        "  model = NN4layerDrop(input_features, 128).to(device)\n",
        "  LR, WEIGHT_DECAY = 1e-4, 0\n",
        "  optimizer = torch.optim.Adam(\n",
        "      model.parameters(),\n",
        "      lr=LR,\n",
        "      weight_decay=WEIGHT_DECAY\n",
        "  )\n",
        "  losses_valid, losses_train, acc_valid, acc_train = fit(model, criterion, optimizer, 500)\n",
        "\n",
        "  plotplz(\"model1.png\")\n",
        "  precision, recall, f1score = show_metrics(model)\n",
        "  foldperf['fold{}'.format(fold+1)] = [sum(losses_valid)/len(losses_valid),\n",
        "                                       sum(losses_train)/len(losses_train),\n",
        "                                       sum(acc_valid)/len(acc_valid),\n",
        "                                       sum(acc_train)/len(acc_train),\n",
        "                                       precision, recall, f1score]\n",
        "\n",
        "  print(\"--------------------------------------------\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rg3-iLM0pSm5",
        "outputId": "bfb3907c-240c-44a1-f6da-f68b628bfbee"
      },
      "outputs": [],
      "source": [
        "cv_losses_valid, cv_losses_train, cv_acc_valid, cv_acc_train = 0, 0, 0, 0\n",
        "cv_precision, cv_recall, cv_f1score = 0, 0, 0\n",
        "for i in range(1, 6):\n",
        "  cv_losses_valid += foldperf[f\"fold{i}\"][0]\n",
        "  cv_losses_train += foldperf[f\"fold{i}\"][1]\n",
        "  cv_acc_valid += foldperf[f\"fold{i}\"][2]\n",
        "  cv_acc_train += foldperf[f\"fold{i}\"][3]\n",
        "  cv_precision += foldperf[f\"fold{i}\"][4]\n",
        "  cv_recall += foldperf[f\"fold{i}\"][5]\n",
        "  cv_f1score += foldperf[f\"fold{i}\"][6]\n",
        "\n",
        "print(\"loss train:\", cv_losses_train/5, \"accuracy train:\", cv_acc_train/5, \"loss valid:\", cv_losses_valid/5, \"accuracy valid\", cv_acc_valid/5)\n",
        "print(\"precision:\", cv_precision/5, \"recall:\", cv_recall/5, \"f1 score:\", cv_f1score/5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pgecb-N92kdg"
      },
      "source": [
        "## Model 2:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PtFnJWzik6qE",
        "outputId": "d3f1173b-be66-4dac-e456-4dc197e467a0"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 16\n",
        "\n",
        "dataset = Dataset(X, y)\n",
        "criterion = nn.CrossEntropyLoss().to(device)\n",
        "\n",
        "class NN4layerNorm(nn.Module):\n",
        "  def __init__(self, input_features, neurons):\n",
        "    super().__init__()\n",
        "    self.neurons = neurons\n",
        "    self.layers = nn.Sequential(\n",
        "        nn.Linear(input_features, self.neurons),\n",
        "        nn.BatchNorm1d(self.neurons),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(self.neurons, self.neurons),\n",
        "        nn.BatchNorm1d(self.neurons),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(self.neurons, self.neurons),\n",
        "        nn.BatchNorm1d(self.neurons),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(self.neurons, 3),\n",
        "    )\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.layers(x)\n",
        "\n",
        "foldperf={}\n",
        "\n",
        "kfold = KFold(n_splits=5, shuffle=True)\n",
        "for fold, (train_ids, test_ids) in enumerate(kfold.split(dataset)):\n",
        "  print(f\"----------------- Fold {fold+1} --------------------\")\n",
        "  train_subsampler = torch.utils.data.SubsetRandomSampler(train_ids)\n",
        "  test_subsampler = torch.utils.data.SubsetRandomSampler(test_ids)\n",
        "  \n",
        "  trainloader = torch.utils.data.DataLoader(\n",
        "                    dataset, \n",
        "                    batch_size=BATCH_SIZE, sampler=train_subsampler)\n",
        "  testloader = torch.utils.data.DataLoader(\n",
        "                    dataset,\n",
        "                    batch_size=BATCH_SIZE, sampler=test_subsampler)\n",
        "  model = NN4layerNorm(input_features, 128).to(device)\n",
        "  LR, WEIGHT_DECAY = 1e-4, 0\n",
        "  optimizer = torch.optim.Adam(\n",
        "      model.parameters(),\n",
        "      lr=LR,\n",
        "      weight_decay=WEIGHT_DECAY\n",
        "  )\n",
        "  losses_valid, losses_train, acc_valid, acc_train = fit(model, criterion, optimizer, 500)\n",
        "\n",
        "  plotplz(\"model1.png\")\n",
        "  precision, recall, f1score = show_metrics(model)\n",
        "  foldperf['fold{}'.format(fold+1)] = [sum(losses_valid)/len(losses_valid),\n",
        "                                       sum(losses_train)/len(losses_train),\n",
        "                                       sum(acc_valid)/len(acc_valid),\n",
        "                                       sum(acc_train)/len(acc_train),\n",
        "                                       precision, recall, f1score]\n",
        "\n",
        "  print(\"--------------------------------------------\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P7l46dOLk6qE"
      },
      "outputs": [],
      "source": [
        "cv_losses_valid, cv_losses_train, cv_acc_valid, cv_acc_train = 0, 0, 0, 0\n",
        "cv_precision, cv_recall, cv_f1score = 0, 0, 0\n",
        "for i in range(1, 6):\n",
        "  cv_losses_valid += foldperf[f\"fold{i}\"][0]\n",
        "  cv_losses_train += foldperf[f\"fold{i}\"][1]\n",
        "  cv_acc_valid += foldperf[f\"fold{i}\"][2]\n",
        "  cv_acc_train += foldperf[f\"fold{i}\"][3]\n",
        "  cv_precision += foldperf[f\"fold{i}\"][4]\n",
        "  cv_recall += foldperf[f\"fold{i}\"][5]\n",
        "  cv_f1score += foldperf[f\"fold{i}\"][6]\n",
        "\n",
        "print(\"loss train:\", cv_losses_train/5, \"accuracy train:\", cv_acc_train/5, \"loss valid:\", cv_losses_valid/5, \"accuracy valid\", cv_acc_valid/5)\n",
        "print(\"precision:\", cv_precision/5, \"recall:\", cv_recall/5, \"f1 score:\", cv_f1score/5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X8FGVaVflLZa"
      },
      "source": [
        "## Model 3:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3FDv4WUwlP5p"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 16\n",
        "\n",
        "dataset = Dataset(X, y)\n",
        "criterion = nn.CrossEntropyLoss().to(device)\n",
        "\n",
        "class NN4layerNorm(nn.Module):\n",
        "  def __init__(self, input_features, neurons):\n",
        "    super().__init__()\n",
        "    self.neurons = neurons\n",
        "    self.layers = nn.Sequential(\n",
        "        nn.Linear(input_features, self.neurons),\n",
        "        nn.BatchNorm1d(self.neurons),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(self.neurons, self.neurons),\n",
        "        nn.BatchNorm1d(self.neurons),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(self.neurons, self.neurons),\n",
        "        nn.BatchNorm1d(self.neurons),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(self.neurons, self.neurons),\n",
        "        nn.BatchNorm1d(self.neurons),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(self.neurons, 3),\n",
        "    )\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.layers(x)\n",
        "\n",
        "foldperf={}\n",
        "\n",
        "kfold = KFold(n_splits=5, shuffle=True)\n",
        "for fold, (train_ids, test_ids) in enumerate(kfold.split(dataset)):\n",
        "  print(f\"----------------- Fold {fold+1} --------------------\")\n",
        "  train_subsampler = torch.utils.data.SubsetRandomSampler(train_ids)\n",
        "  test_subsampler = torch.utils.data.SubsetRandomSampler(test_ids)\n",
        "  \n",
        "  trainloader = torch.utils.data.DataLoader(\n",
        "                    dataset, \n",
        "                    batch_size=BATCH_SIZE, sampler=train_subsampler)\n",
        "  testloader = torch.utils.data.DataLoader(\n",
        "                    dataset,\n",
        "                    batch_size=BATCH_SIZE, sampler=test_subsampler)\n",
        "  model = NN4layerNorm(input_features, 128).to(device)\n",
        "  LR, WEIGHT_DECAY = 1e-4, 0\n",
        "  optimizer = torch.optim.Adam(\n",
        "      model.parameters(),\n",
        "      lr=LR,\n",
        "      weight_decay=WEIGHT_DECAY\n",
        "  )\n",
        "  losses_valid, losses_train, acc_valid, acc_train = fit(model, criterion, optimizer, 500)\n",
        "\n",
        "  plotplz(\"model1.png\")\n",
        "  precision, recall, f1score = show_metrics(model)\n",
        "  foldperf['fold{}'.format(fold+1)] = [sum(losses_valid)/len(losses_valid),\n",
        "                                       sum(losses_train)/len(losses_train),\n",
        "                                       sum(acc_valid)/len(acc_valid),\n",
        "                                       sum(acc_train)/len(acc_train),\n",
        "                                       precision, recall, f1score]\n",
        "\n",
        "  print(\"--------------------------------------------\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "_FRhXsqOlP5q",
        "outputId": "d7d08920-558d-49d8-b931-12c69ebf060f"
      },
      "outputs": [],
      "source": [
        "cv_losses_valid, cv_losses_train, cv_acc_valid, cv_acc_train = 0, 0, 0, 0\n",
        "cv_precision, cv_recall, cv_f1score = 0, 0, 0\n",
        "for i in range(1, 6):\n",
        "  cv_losses_valid += foldperf[f\"fold{i}\"][0]\n",
        "  cv_losses_train += foldperf[f\"fold{i}\"][1]\n",
        "  cv_acc_valid += foldperf[f\"fold{i}\"][2]\n",
        "  cv_acc_train += foldperf[f\"fold{i}\"][3]\n",
        "  cv_precision += foldperf[f\"fold{i}\"][4]\n",
        "  cv_recall += foldperf[f\"fold{i}\"][5]\n",
        "  cv_f1score += foldperf[f\"fold{i}\"][6]\n",
        "\n",
        "print(\"loss train:\", cv_losses_train/5, \"accuracy train:\", cv_acc_train/5, \"loss valid:\", cv_losses_valid/5, \"accuracy valid\", cv_acc_valid/5)\n",
        "print(\"precision:\", cv_precision/5, \"recall:\", cv_recall/5, \"f1 score:\", cv_f1score/5)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Deep Learning - Project 2",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
